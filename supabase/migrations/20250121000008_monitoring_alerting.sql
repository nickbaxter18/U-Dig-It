-- Comprehensive Monitoring and Alerting System for Kubota Rental Platform
-- This migration adds sophisticated monitoring, alerting, and cost optimization features
-- Created: 2025-01-21

-- Enable advanced monitoring extensions
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";
CREATE EXTENSION IF NOT EXISTS "pg_buffercache";
CREATE EXTENSION IF NOT EXISTS "pg_prewarm";

-- Cost Optimization and Usage Tracking
CREATE TABLE usage_costs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  date DATE NOT NULL,
  category VARCHAR(50) NOT NULL, -- 'storage', 'compute', 'bandwidth', 'api_calls'
  subcategory VARCHAR(100),
  units DECIMAL(15,4) NOT NULL,
  unit_cost DECIMAL(10,6) NOT NULL,
  total_cost DECIMAL(12,4) NOT NULL,
  currency VARCHAR(3) DEFAULT 'CAD',
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  UNIQUE(date, category, subcategory)
);

-- Alert Configuration System
CREATE TABLE alert_rules (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name VARCHAR(100) UNIQUE NOT NULL,
  description TEXT,
  metric_name VARCHAR(100) NOT NULL,
  alert_type VARCHAR(50) NOT NULL, -- 'threshold', 'anomaly', 'rate_change'
  condition_operator VARCHAR(10) NOT NULL, -- '>', '<', '=', '!=', 'between'
  condition_value DECIMAL(15,4),
  condition_value2 DECIMAL(15,4), -- For 'between' conditions
  evaluation_window_minutes INTEGER DEFAULT 60,
  cooldown_minutes INTEGER DEFAULT 60,
  severity VARCHAR(20) NOT NULL, -- 'info', 'warning', 'error', 'critical'
  is_active BOOLEAN DEFAULT true,
  notification_channels TEXT[] DEFAULT ARRAY['email'],
  created_by UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Alert History and Status
CREATE TABLE alert_incidents (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  alert_rule_id UUID NOT NULL REFERENCES alert_rules(id) ON DELETE CASCADE,
  alert_name VARCHAR(100) NOT NULL,
  severity VARCHAR(20) NOT NULL,
  status VARCHAR(20) DEFAULT 'open', -- 'open', 'acknowledged', 'resolved', 'suppressed'
  metric_name VARCHAR(100) NOT NULL,
  current_value DECIMAL(15,4),
  threshold_value DECIMAL(15,4),
  description TEXT,
  triggered_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  acknowledged_at TIMESTAMP WITH TIME ZONE,
  acknowledged_by UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  resolved_at TIMESTAMP WITH TIME ZONE,
  resolved_by UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  resolution_notes TEXT,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- System Metrics Collection
CREATE TABLE system_metrics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  metric_name VARCHAR(100) NOT NULL,
  metric_category VARCHAR(50) NOT NULL,
  metric_value DECIMAL(15,4) NOT NULL,
  unit VARCHAR(20),
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Database Performance Metrics
CREATE TABLE database_performance_metrics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  active_connections INTEGER,
  idle_connections INTEGER,
  waiting_connections INTEGER,
  total_connections INTEGER,
  cache_hit_ratio DECIMAL(5,4),
  avg_query_time_ms DECIMAL(10,2),
  slow_queries_count INTEGER,
  deadlocks_count INTEGER,
  temp_files_created INTEGER,
  temp_bytes_written BIGINT,
  buffer_cache_hit_ratio DECIMAL(5,4),
  shared_buffers_used_mb INTEGER,
  wal_generated_bytes BIGINT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- API Performance and Usage Analytics
CREATE TABLE api_analytics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  endpoint VARCHAR(500) NOT NULL,
  method VARCHAR(10) NOT NULL,
  user_id UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  response_time_ms INTEGER NOT NULL,
  status_code INTEGER NOT NULL,
  request_size_bytes INTEGER,
  response_size_bytes INTEGER,
  user_agent TEXT,
  ip_address INET,
  country_code VARCHAR(2),
  device_type VARCHAR(50),
  browser_name VARCHAR(50),
  error_type VARCHAR(100),
  cache_hit BOOLEAN DEFAULT false,
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Customer Behavior Analytics
CREATE TABLE customer_behavior_analytics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  customer_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  session_id VARCHAR(255),
  event_type VARCHAR(50) NOT NULL, -- 'page_view', 'booking_start', 'booking_complete', 'payment_attempt'
  event_name VARCHAR(100) NOT NULL,
  event_data JSONB,
  page_url TEXT,
  referrer_url TEXT,
  time_on_page_seconds INTEGER,
  scroll_depth_percentage DECIMAL(5,2),
  conversion_funnel_stage VARCHAR(50),
  device_type VARCHAR(50),
  browser_name VARCHAR(50),
  screen_resolution VARCHAR(20),
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Error Tracking and Analysis
CREATE TABLE error_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  error_type VARCHAR(50) NOT NULL,
  error_level VARCHAR(20) NOT NULL, -- 'debug', 'info', 'warning', 'error', 'critical'
  error_message TEXT NOT NULL,
  error_stack TEXT,
  error_code VARCHAR(50),
  user_id UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  session_id VARCHAR(255),
  request_url TEXT,
  request_method VARCHAR(10),
  request_headers JSONB,
  request_body TEXT,
  response_status INTEGER,
  user_agent TEXT,
  ip_address INET,
  browser_info JSONB,
  device_info JSONB,
  location_data JSONB,
  environment VARCHAR(20) DEFAULT 'production',
  version VARCHAR(20),
  resolved BOOLEAN DEFAULT false,
  resolved_at TIMESTAMP WITH TIME ZONE,
  resolution_notes TEXT,
  occurrence_count INTEGER DEFAULT 1,
  first_occurrence TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_occurrence TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Feature Usage Analytics
CREATE TABLE feature_usage_analytics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  feature_name VARCHAR(100) NOT NULL,
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  usage_count INTEGER DEFAULT 1,
  first_used TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_used TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  total_time_spent_seconds INTEGER DEFAULT 0,
  success_rate DECIMAL(5,4) DEFAULT 1.0,
  error_count INTEGER DEFAULT 0,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- A/B Testing Framework
CREATE TABLE ab_tests (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  test_name VARCHAR(100) UNIQUE NOT NULL,
  description TEXT,
  feature_name VARCHAR(100) NOT NULL,
  hypothesis TEXT,
  status VARCHAR(20) DEFAULT 'draft', -- 'draft', 'running', 'paused', 'completed'
  start_date TIMESTAMP WITH TIME ZONE,
  end_date TIMESTAMP WITH TIME ZONE,
  variants JSONB NOT NULL,
  traffic_split JSONB NOT NULL,
  primary_metric VARCHAR(100) NOT NULL,
  secondary_metrics TEXT[],
  minimum_sample_size INTEGER DEFAULT 1000,
  confidence_level DECIMAL(3,2) DEFAULT 0.95,
  statistical_significance DECIMAL(5,4),
  winner_variant VARCHAR(50),
  results JSONB,
  created_by UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Conversion Funnel Analytics
CREATE TABLE conversion_funnels (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  funnel_name VARCHAR(100) NOT NULL,
  funnel_stages JSONB NOT NULL,
  session_id VARCHAR(255),
  user_id UUID REFERENCES auth.users(id) ON DELETE SET NULL,
  current_stage VARCHAR(100),
  stage_progress JSONB,
  conversion_rate DECIMAL(5,4),
  time_to_convert_seconds INTEGER,
  abandoned_at_stage VARCHAR(100),
  completed_at TIMESTAMP WITH TIME ZONE,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create comprehensive indexes for monitoring tables
CREATE INDEX CONCURRENTLY idx_usage_costs_date_category ON usage_costs(date, category, subcategory);
CREATE INDEX CONCURRENTLY idx_alert_rules_active ON alert_rules(is_active, metric_name, severity);
CREATE INDEX CONCURRENTLY idx_alert_incidents_status ON alert_incidents(status, severity, triggered_at);
CREATE INDEX CONCURRENTLY idx_system_metrics_category_timestamp ON system_metrics(metric_category, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_database_performance_timestamp ON database_performance_metrics(timestamp DESC);
CREATE INDEX CONCURRENTLY idx_api_analytics_endpoint_timestamp ON api_analytics(endpoint, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_api_analytics_user_timestamp ON api_analytics(user_id, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_customer_behavior_user_timestamp ON customer_behavior_analytics(customer_id, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_customer_behavior_event_type ON customer_behavior_analytics(event_type, timestamp DESC);
CREATE INDEX CONCURRENTLY idx_error_logs_type_level ON error_logs(error_type, error_level, created_at DESC);
CREATE INDEX CONCURRENTLY idx_error_logs_user_timestamp ON error_logs(user_id, created_at DESC);
CREATE INDEX CONCURRENTLY idx_feature_usage_user_feature ON feature_usage_analytics(user_id, feature_name, last_used);
CREATE INDEX CONCURRENTLY idx_ab_tests_status_dates ON ab_tests(status, start_date, end_date);
CREATE INDEX CONCURRENTLY idx_conversion_funnels_user_stage ON conversion_funnels(user_id, current_stage, completed_at);

-- GIN indexes for JSONB and array fields
CREATE INDEX CONCURRENTLY idx_usage_costs_metadata ON usage_costs USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_alert_incidents_metadata ON alert_incidents USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_system_metrics_metadata ON system_metrics USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_api_analytics_metadata ON api_analytics USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_customer_behavior_data ON customer_behavior_analytics USING gin(event_data);
CREATE INDEX CONCURRENTLY idx_error_logs_metadata ON error_logs USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_feature_usage_metadata ON feature_usage_analytics USING gin(metadata);
CREATE INDEX CONCURRENTLY idx_ab_tests_variants ON ab_tests USING gin(variants);
CREATE INDEX CONCURRENTLY idx_conversion_funnels_progress ON conversion_funnels USING gin(stage_progress);

-- Function to calculate and log usage costs
CREATE OR REPLACE FUNCTION calculate_usage_costs()
RETURNS TABLE (
  date DATE,
  category VARCHAR,
  subcategory VARCHAR,
  units DECIMAL,
  unit_cost DECIMAL,
  total_cost DECIMAL
) AS $$
DECLARE
  v_date DATE := CURRENT_DATE - 1;
  v_storage_cost_per_gb DECIMAL(10,6) := 0.023; -- Supabase storage cost
  v_compute_cost_per_hour DECIMAL(10,6) := 0.125; -- Supabase compute cost
  v_bandwidth_cost_per_gb DECIMAL(10,6) := 0.09; -- Supabase bandwidth cost
  v_api_cost_per_request DECIMAL(10,8) := 0.0001; -- Estimated API cost
BEGIN
  RETURN QUERY
  -- Storage costs
  SELECT
    v_date,
    'storage'::VARCHAR,
    t.table_name::VARCHAR,
    (pg_total_relation_size(schemaname || '.' || tablename) / 1024.0 / 1024.0 / 1024.0)::DECIMAL,
    v_storage_cost_per_gb,
    (pg_total_relation_size(schemaname || '.' || tablename) / 1024.0 / 1024.0 / 1024.0 * v_storage_cost_per_gb)::DECIMAL
  FROM pg_tables t
  JOIN pg_stat_user_tables s ON t.tablename = s.relname
  WHERE t.schemaname = 'public'

  UNION ALL

  -- API usage costs
  SELECT
    v_date,
    'api_calls'::VARCHAR,
    'database_queries'::VARCHAR,
    COUNT(*)::DECIMAL,
    v_api_cost_per_request,
    (COUNT(*) * v_api_cost_per_request)::DECIMAL
  FROM query_performance_log
  WHERE execution_start::date = v_date

  UNION ALL

  -- Compute costs (estimated based on query complexity)
  SELECT
    v_date,
    'compute'::VARCHAR,
    'query_processing'::VARCHAR,
    SUM(execution_time_ms)::DECIMAL / 1000 / 3600, -- Convert to hours
    v_compute_cost_per_hour,
    (SUM(execution_time_ms)::DECIMAL / 1000 / 3600 * v_compute_cost_per_hour)::DECIMAL
  FROM query_performance_log
  WHERE execution_start::date = v_date;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to evaluate alert rules and create incidents
CREATE OR REPLACE FUNCTION evaluate_alert_rules()
RETURNS TABLE (
  alert_rule_id UUID,
  alert_name VARCHAR,
  severity VARCHAR,
  current_value DECIMAL,
  threshold_value DECIMAL,
  incident_created BOOLEAN
) AS $$
DECLARE
  v_rule RECORD;
  v_current_value DECIMAL(15,4);
  v_should_alert BOOLEAN := false;
  v_incident_id UUID;
BEGIN
  FOR v_rule IN
    SELECT * FROM alert_rules
    WHERE is_active = true
      AND (last_evaluated IS NULL OR last_evaluated < CURRENT_TIMESTAMP - INTERVAL '1 minute' * evaluation_window_minutes)
  LOOP
    -- Get current metric value (simplified implementation)
    -- In production, this would be more sophisticated
    IF v_rule.metric_name = 'avg_query_time' THEN
      SELECT AVG(execution_time_ms) INTO v_current_value
      FROM query_performance_log
      WHERE execution_start >= CURRENT_TIMESTAMP - INTERVAL '1 hour';
    ELSIF v_rule.metric_name = 'error_rate' THEN
      SELECT
        (COUNT(*) FILTER (WHERE status_code >= 400)::DECIMAL / COUNT(*)::DECIMAL) * 100
      INTO v_current_value
      FROM api_analytics
      WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour';
    ELSIF v_rule.metric_name = 'active_connections' THEN
      SELECT COUNT(*) INTO v_current_value
      FROM pg_stat_activity
      WHERE state = 'active';
    END IF;

    -- Evaluate condition
    v_should_alert := CASE v_rule.condition_operator
      WHEN '>' THEN v_current_value > v_rule.condition_value
      WHEN '<' THEN v_current_value < v_rule.condition_value
      WHEN '=' THEN v_current_value = v_rule.condition_value
      WHEN '!=' THEN v_current_value != v_rule.condition_value
      WHEN 'between' THEN v_current_value BETWEEN v_rule.condition_value AND v_rule.condition_value2
      ELSE false
    END;

    IF v_should_alert THEN
      -- Check if we're in cooldown period
      IF NOT EXISTS (
        SELECT 1 FROM alert_incidents
        WHERE alert_rule_id = v_rule.id
          AND status = 'open'
          AND triggered_at > CURRENT_TIMESTAMP - INTERVAL '1 minute' * v_rule.cooldown_minutes
      ) THEN
        -- Create alert incident
        INSERT INTO alert_incidents (
          alert_rule_id,
          alert_name,
          severity,
          metric_name,
          current_value,
          threshold_value,
          description
        ) VALUES (
          v_rule.id,
          v_rule.name,
          v_rule.severity,
          v_rule.metric_name,
          v_current_value,
          v_rule.condition_value,
          'Alert triggered: ' || v_rule.metric_name || ' ' || v_rule.condition_operator || ' ' || v_rule.condition_value
        ) RETURNING id INTO v_incident_id;

        -- Send notifications (simplified)
        INSERT INTO notifications (
          user_id,
          type,
          status,
          priority,
          title,
          message,
          metadata
        )
        SELECT
          u.id,
          'email',
          'pending',
          CASE v_rule.severity
            WHEN 'critical' THEN 'critical'
            WHEN 'error' THEN 'high'
            WHEN 'warning' THEN 'medium'
            ELSE 'low'
          END,
          'System Alert: ' || v_rule.name,
          'Alert triggered for ' || v_rule.metric_name || '. Current: ' || v_current_value || ', Threshold: ' || v_rule.condition_value,
          jsonb_build_object('alert_id', v_incident_id, 'automated', true)
        FROM users u
        WHERE u.role IN ('admin', 'super_admin');

        alert_rule_id := v_rule.id;
        alert_name := v_rule.name;
        severity := v_rule.severity;
        current_value := v_current_value;
        threshold_value := v_rule.condition_value;
        incident_created := true;

        RETURN NEXT;
      END IF;
    END IF;

    -- Update last evaluated timestamp
    UPDATE alert_rules
    SET last_evaluated = CURRENT_TIMESTAMP
    WHERE id = v_rule.id;
  END LOOP;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to collect system metrics
CREATE OR REPLACE FUNCTION collect_system_metrics()
RETURNS void AS $$
DECLARE
  v_cache_hit DECIMAL(5,4);
  v_buffer_hit DECIMAL(5,4);
  v_avg_query_time DECIMAL(10,2);
  v_slow_queries INTEGER;
  v_connections_active INTEGER;
  v_connections_idle INTEGER;
  v_connections_waiting INTEGER;
  v_deadlocks INTEGER;
  v_temp_files INTEGER;
  v_temp_bytes BIGINT;
BEGIN
  -- Get cache hit ratios
  SELECT
    CASE
      WHEN sum(heap_blks_read) + sum(heap_blks_hit) > 0
      THEN sum(heap_blks_hit)::DECIMAL / (sum(heap_blks_read) + sum(heap_blks_hit))
      ELSE 0
    END INTO v_cache_hit
  FROM pg_stat_user_tables;

  SELECT
    CASE
      WHEN sum(buffers_alloc) > 0
      THEN (sum(buffers_alloc) - sum(buffers_backend_fsync))::DECIMAL / sum(buffers_alloc)
      ELSE 0
    END INTO v_buffer_hit
  FROM pg_stat_bgwriter;

  -- Get query performance metrics
  SELECT AVG(execution_time_ms) INTO v_avg_query_time
  FROM query_performance_log
  WHERE execution_start >= CURRENT_TIMESTAMP - INTERVAL '1 hour';

  SELECT COUNT(*) INTO v_slow_queries
  FROM query_performance_log
  WHERE execution_time_ms > 1000
    AND execution_start >= CURRENT_TIMESTAMP - INTERVAL '1 hour';

  -- Get connection metrics
  SELECT
    COUNT(*) FILTER (WHERE state = 'active') as active,
    COUNT(*) FILTER (WHERE state = 'idle') as idle,
    COUNT(*) FILTER (WHERE wait_event IS NOT NULL) as waiting
  INTO v_connections_active, v_connections_idle, v_connections_waiting
  FROM pg_stat_activity;

  -- Insert performance metrics
  INSERT INTO database_performance_metrics (
    active_connections,
    idle_connections,
    waiting_connections,
    total_connections,
    cache_hit_ratio,
    avg_query_time_ms,
    slow_queries_count,
    buffer_cache_hit_ratio
  ) VALUES (
    v_connections_active,
    v_connections_idle,
    v_connections_waiting,
    v_connections_active + v_connections_idle + v_connections_waiting,
    v_cache_hit,
    v_avg_query_time,
    v_slow_queries,
    v_buffer_hit
  );
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to detect performance anomalies
CREATE OR REPLACE FUNCTION detect_performance_anomalies()
RETURNS TABLE (
  anomaly_type VARCHAR,
  metric_name VARCHAR,
  current_value DECIMAL,
  baseline_value DECIMAL,
  deviation_percentage DECIMAL,
  severity VARCHAR,
  recommendation TEXT
) AS $$
DECLARE
  v_current_avg DECIMAL(10,2);
  v_baseline_avg DECIMAL(10,2);
  v_deviation DECIMAL(8,2);
BEGIN
  -- Check query performance anomaly
  SELECT AVG(execution_time_ms) INTO v_current_avg
  FROM query_performance_log
  WHERE execution_start >= CURRENT_TIMESTAMP - INTERVAL '1 hour';

  SELECT AVG(execution_time_ms) INTO v_baseline_avg
  FROM query_performance_log
  WHERE execution_start BETWEEN CURRENT_TIMESTAMP - INTERVAL '25 hours' AND CURRENT_TIMESTAMP - INTERVAL '1 hour';

  IF v_baseline_avg > 0 THEN
    v_deviation := ((v_current_avg - v_baseline_avg) / v_baseline_avg) * 100;

    IF ABS(v_deviation) > 50 THEN
      RETURN QUERY SELECT
        'Query Performance'::VARCHAR,
        'avg_execution_time'::VARCHAR,
        v_current_avg,
        v_baseline_avg,
        v_deviation,
        CASE
          WHEN ABS(v_deviation) > 100 THEN 'critical'
          WHEN ABS(v_deviation) > 50 THEN 'warning'
          ELSE 'info'
        END,
        'Query performance has deviated significantly. Check for slow queries or database load.';
    END IF;
  END IF;

  -- Check connection anomaly
  SELECT COUNT(*)::DECIMAL INTO v_current_avg
  FROM pg_stat_activity
  WHERE state = 'active';

  SELECT AVG(active_connections) INTO v_baseline_avg
  FROM database_performance_metrics
  WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '24 hours';

  IF v_baseline_avg > 0 THEN
    v_deviation := ((v_current_avg - v_baseline_avg) / v_baseline_avg) * 100;

    IF ABS(v_deviation) > 30 THEN
      RETURN QUERY SELECT
        'Connection Pool'::VARCHAR,
        'active_connections'::VARCHAR,
        v_current_avg,
        v_baseline_avg,
        v_deviation,
        CASE
          WHEN ABS(v_deviation) > 100 THEN 'critical'
          WHEN ABS(v_deviation) > 50 THEN 'warning'
          ELSE 'info'
        END,
        'Connection count has deviated significantly. Check application connection management.';
    END IF;
  END IF;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to generate cost optimization recommendations
CREATE OR REPLACE FUNCTION generate_cost_optimization_recommendations()
RETURNS TABLE (
  category VARCHAR,
  recommendation VARCHAR,
  potential_savings DECIMAL,
  implementation_effort VARCHAR,
  priority INTEGER,
  details TEXT
) AS $$
DECLARE
  v_total_cost DECIMAL(12,4);
  v_query_cost DECIMAL(12,4);
  v_storage_cost DECIMAL(12,4);
  v_index_cost DECIMAL(12,4);
BEGIN
  -- Calculate current costs
  SELECT SUM(total_cost) INTO v_total_cost FROM usage_costs WHERE date >= CURRENT_DATE - INTERVAL '30 days';
  SELECT SUM(total_cost) INTO v_query_cost FROM usage_costs WHERE category = 'api_calls' AND date >= CURRENT_DATE - INTERVAL '30 days';
  SELECT SUM(total_cost) INTO v_storage_cost FROM usage_costs WHERE category = 'storage' AND date >= CURRENT_DATE - INTERVAL '30 days';

  RETURN QUERY
  -- Query optimization recommendations
  SELECT
    'Query Performance'::VARCHAR,
    'Implement query result caching for frequently accessed data'::VARCHAR,
    v_query_cost * 0.3,
    'Medium'::VARCHAR,
    7,
    'Add caching for equipment availability and customer analytics queries'
  WHERE v_query_cost > 10

  UNION ALL

  -- Storage optimization
  SELECT
    'Storage'::VARCHAR,
    'Review and optimize large table storage'::VARCHAR,
    v_storage_cost * 0.2,
    'Low'::VARCHAR,
    6,
    'Implement table partitioning for audit_logs and api_usage tables'
  WHERE v_storage_cost > 50

  UNION ALL

  -- Index optimization
  SELECT
    'Database'::VARCHAR,
    'Remove unused or low-usage indexes'::VARCHAR,
    v_index_cost * 0.15,
    'Low'::VARCHAR,
    5,
    'Run detect_unused_indexes() function and remove unnecessary indexes'
  WHERE v_index_cost > 20

  UNION ALL

  -- API optimization
  SELECT
    'API Usage'::VARCHAR,
    'Implement request deduplication and batching'::VARCHAR,
    v_query_cost * 0.4,
    'High'::VARCHAR,
    8,
    'Add request caching and batch similar API calls together'
  WHERE v_query_cost > 25

  UNION ALL

  -- Archive old data
  SELECT
    'Data Management'::VARCHAR,
    'Archive historical data older than 2 years'::VARCHAR,
    v_storage_cost * 0.25,
    'Medium'::VARCHAR,
    6,
    'Move old bookings and audit data to cold storage'
  WHERE v_storage_cost > 100

  UNION ALL

  -- Connection pooling optimization
  SELECT
    'Connection Management'::VARCHAR,
    'Optimize connection pool settings'::VARCHAR,
    (v_total_cost * 0.1),
    'Low'::VARCHAR,
    4,
    'Adjust max_connections and connection timeout settings'
  WHERE v_total_cost > 200;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to track API analytics
CREATE OR REPLACE FUNCTION track_api_analytics(
  p_endpoint VARCHAR,
  p_method VARCHAR,
  p_user_id UUID,
  p_response_time_ms INTEGER,
  p_status_code INTEGER,
  p_request_size INTEGER DEFAULT NULL,
  p_response_size INTEGER DEFAULT NULL,
  p_user_agent TEXT DEFAULT NULL,
  p_error_type VARCHAR DEFAULT NULL
)
RETURNS void AS $$
DECLARE
  v_country_code VARCHAR(2);
  v_device_type VARCHAR(50);
  v_browser_name VARCHAR(50);
BEGIN
  -- Extract metadata from user agent (simplified)
  v_device_type := CASE
    WHEN p_user_agent LIKE '%Mobile%' THEN 'mobile'
    WHEN p_user_agent LIKE '%Tablet%' THEN 'tablet'
    ELSE 'desktop'
  END;

  v_browser_name := CASE
    WHEN p_user_agent LIKE '%Chrome%' THEN 'Chrome'
    WHEN p_user_agent LIKE '%Firefox%' THEN 'Firefox'
    WHEN p_user_agent LIKE '%Safari%' THEN 'Safari'
    WHEN p_user_agent LIKE '%Edge%' THEN 'Edge'
    ELSE 'Unknown'
  END;

  -- Insert API analytics record
  INSERT INTO api_analytics (
    endpoint,
    method,
    user_id,
    response_time_ms,
    status_code,
    request_size_bytes,
    response_size_bytes,
    user_agent,
    device_type,
    browser_name,
    error_type,
    cache_hit
  ) VALUES (
    p_endpoint,
    p_method,
    p_user_id,
    p_response_time_ms,
    p_status_code,
    p_request_size,
    p_response_size,
    p_user_agent,
    v_device_type,
    v_browser_name,
    p_error_type,
    (p_response_time_ms < 50) -- Assume cache hit if very fast
  );

  -- Also update api_usage table for backward compatibility
  INSERT INTO api_usage (
    user_id,
    endpoint,
    method,
    status_code,
    response_time_ms,
    metadata
  ) VALUES (
    p_user_id,
    p_endpoint,
    p_method,
    p_status_code,
    p_response_time_ms,
    jsonb_build_object(
      'device_type', v_device_type,
      'browser', v_browser_name,
      'cache_hit', (p_response_time_ms < 50)
    )
  );
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to track customer behavior
CREATE OR REPLACE FUNCTION track_customer_behavior(
  p_customer_id UUID,
  p_event_type VARCHAR,
  p_event_name VARCHAR,
  p_event_data JSONB DEFAULT NULL,
  p_page_url TEXT DEFAULT NULL,
  p_referrer_url TEXT DEFAULT NULL,
  p_session_id VARCHAR DEFAULT NULL,
  p_time_on_page INTEGER DEFAULT NULL,
  p_scroll_depth DECIMAL DEFAULT NULL
)
RETURNS void AS $$
DECLARE
  v_conversion_stage VARCHAR(50);
BEGIN
  -- Determine conversion funnel stage
  v_conversion_stage := CASE
    WHEN p_event_name LIKE '%equipment%' AND p_event_name LIKE '%view%' THEN 'awareness'
    WHEN p_event_name LIKE '%booking%' AND p_event_name LIKE '%start%' THEN 'interest'
    WHEN p_event_name LIKE '%booking%' AND p_event_name LIKE '%complete%' THEN 'purchase'
    WHEN p_event_name LIKE '%payment%' THEN 'payment'
    ELSE 'other'
  END;

  -- Insert behavior analytics
  INSERT INTO customer_behavior_analytics (
    customer_id,
    session_id,
    event_type,
    event_name,
    event_data,
    page_url,
    referrer_url,
    time_on_page_seconds,
    scroll_depth_percentage,
    conversion_funnel_stage
  ) VALUES (
    p_customer_id,
    p_session_id,
    p_event_type,
    p_event_name,
    p_event_data,
    p_page_url,
    p_referrer_url,
    p_time_on_page,
    p_scroll_depth,
    v_conversion_stage
  );
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to log and analyze errors
CREATE OR REPLACE FUNCTION log_error(
  p_error_type VARCHAR,
  p_error_level VARCHAR,
  p_error_message TEXT,
  p_error_stack TEXT DEFAULT NULL,
  p_user_id UUID DEFAULT NULL,
  p_session_id VARCHAR DEFAULT NULL,
  p_request_url TEXT DEFAULT NULL,
  p_request_method VARCHAR DEFAULT NULL,
  p_response_status INTEGER DEFAULT NULL,
  p_user_agent TEXT DEFAULT NULL,
  p_environment VARCHAR DEFAULT 'production'
)
RETURNS UUID AS $$
DECLARE
  v_error_id UUID;
  v_existing_count INTEGER;
BEGIN
  -- Check if this is a duplicate error (same type, message, user within 5 minutes)
  SELECT COUNT(*) INTO v_existing_count
  FROM error_logs
  WHERE error_type = p_error_type
    AND error_message = p_error_message
    AND user_id = p_user_id
    AND created_at >= CURRENT_TIMESTAMP - INTERVAL '5 minutes';

  IF v_existing_count > 0 THEN
    -- Update existing error count
    UPDATE error_logs
    SET
      occurrence_count = occurrence_count + 1,
      last_occurrence = CURRENT_TIMESTAMP,
      updated_at = NOW()
    WHERE error_type = p_error_type
      AND error_message = p_error_message
      AND user_id = p_user_id
      AND created_at >= CURRENT_TIMESTAMP - INTERVAL '5 minutes'
    RETURNING id INTO v_error_id;
  ELSE
    -- Insert new error
    INSERT INTO error_logs (
      error_type,
      error_level,
      error_message,
      error_stack,
      user_id,
      session_id,
      request_url,
      request_method,
      response_status,
      user_agent,
      environment
    ) VALUES (
      p_error_type,
      p_error_level,
      p_error_message,
      p_error_stack,
      p_user_id,
      p_session_id,
      p_request_url,
      p_request_method,
      p_response_status,
      p_user_agent,
      p_environment
    ) RETURNING id INTO v_error_id;

    -- Create alert for critical errors
    IF p_error_level IN ('error', 'critical') THEN
      INSERT INTO alert_incidents (
        alert_rule_id,
        alert_name,
        severity,
        metric_name,
        current_value,
        threshold_value,
        description,
        metadata
      )
      SELECT
        NULL::UUID,
        'System Error Alert'::VARCHAR,
        CASE p_error_level
          WHEN 'critical' THEN 'critical'
          ELSE 'error'
        END,
        'error_count'::VARCHAR,
        1::DECIMAL,
        0::DECIMAL,
        'Critical system error: ' || p_error_message,
        jsonb_build_object('error_id', v_error_id, 'error_type', p_error_type, 'automated', true)
      WHERE NOT EXISTS (
        SELECT 1 FROM alert_incidents
        WHERE metadata->>'error_id' = v_error_id::text
      );
    END IF;
  END IF;

  RETURN v_error_id;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to calculate conversion funnel metrics
CREATE OR REPLACE FUNCTION calculate_conversion_funnel()
RETURNS TABLE (
  funnel_stage VARCHAR,
  users_count INTEGER,
  conversion_rate DECIMAL,
  avg_time_to_stage DECIMAL,
  dropout_rate DECIMAL
) AS $$
DECLARE
  v_total_users INTEGER;
  v_stage_data RECORD;
BEGIN
  -- Get total users who started the funnel
  SELECT COUNT(DISTINCT user_id) INTO v_total_users
  FROM customer_behavior_analytics
  WHERE event_type = 'booking'
    AND timestamp >= CURRENT_DATE - INTERVAL '30 days';

  FOR v_stage_data IN
    SELECT
      conversion_funnel_stage,
      COUNT(DISTINCT user_id) as users_in_stage,
      AVG(time_on_page_seconds) as avg_time_seconds
    FROM customer_behavior_analytics
    WHERE timestamp >= CURRENT_DATE - INTERVAL '30 days'
      AND conversion_funnel_stage IS NOT NULL
    GROUP BY conversion_funnel_stage
    ORDER BY
      CASE conversion_funnel_stage
        WHEN 'awareness' THEN 1
        WHEN 'interest' THEN 2
        WHEN 'purchase' THEN 3
        WHEN 'payment' THEN 4
        ELSE 5
      END
  LOOP
    RETURN QUERY SELECT
      v_stage_data.conversion_funnel_stage,
      v_stage_data.users_in_stage,
      CASE
        WHEN v_total_users > 0 THEN (v_stage_data.users_in_stage::DECIMAL / v_total_users) * 100
        ELSE 0
      END,
      v_stage_data.avg_time_seconds,
      CASE
        WHEN v_total_users > 0 THEN ((v_total_users - v_stage_data.users_in_stage)::DECIMAL / v_total_users) * 100
        ELSE 0
      END;
  END LOOP;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to optimize database performance based on metrics
CREATE OR REPLACE FUNCTION optimize_database_performance()
RETURNS TABLE (
  optimization_type VARCHAR,
  action_taken VARCHAR,
  impact VARCHAR,
  details TEXT
) AS $$
DECLARE
  v_slow_query RECORD;
  v_unused_index RECORD;
  v_table_stats RECORD;
BEGIN
  -- Check for slow queries and suggest optimizations
  FOR v_slow_query IN
    SELECT DISTINCT query_hash, query_text, AVG(execution_time_ms) as avg_time
    FROM query_performance_log
    WHERE execution_time_ms > 1000
      AND execution_start >= CURRENT_DATE - INTERVAL '1 day'
    GROUP BY query_hash, query_text
    ORDER BY avg_time DESC
    LIMIT 5
  LOOP
    -- This would implement actual query optimization in production
    RETURN QUERY SELECT
      'Query Optimization'::VARCHAR,
      'Analyzed slow query'::VARCHAR,
      'Medium'::VARCHAR,
      'Query hash: ' || v_slow_query.query_hash || ', Avg time: ' || v_slow_query.avg_time || 'ms';
  END LOOP;

  -- Check for unused indexes
  FOR v_unused_index IN
    SELECT * FROM detect_unused_indexes(7)
    WHERE recommendation LIKE 'Consider dropping%'
    LIMIT 3
  LOOP
    RETURN QUERY SELECT
      'Index Optimization'::VARCHAR,
      'Identified unused index'::VARCHAR,
      'Low'::VARCHAR,
      'Index: ' || v_unused_index.index_name || ' on ' || v_unused_index.table_name || ' (Size: ' || v_unused_index.index_size_mb || 'MB)';
  END LOOP;

  -- Check table statistics and suggest maintenance
  FOR v_table_stats IN
    SELECT * FROM optimize_table_statistics()
    WHERE last_analyze < CURRENT_DATE - INTERVAL '7 days'
       OR n_dead_tup > n_live_tup * 0.2
    LIMIT 5
  LOOP
    RETURN QUERY SELECT
      'Table Maintenance'::VARCHAR,
      'Statistics update needed'::VARCHAR,
      'Low'::VARCHAR,
      'Table: ' || v_table_stats.table_name || ', Dead tuples: ' || v_table_stats.n_dead_tup;
  END LOOP;

  -- Always return at least one record
  IF NOT FOUND THEN
    RETURN QUERY SELECT
      'Performance'::VARCHAR,
      'No optimizations needed'::VARCHAR,
      'None'::VARCHAR,
      'Database performance is optimal';
  END IF;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to generate business insights
CREATE OR REPLACE FUNCTION generate_business_insights()
RETURNS TABLE (
  insight_type VARCHAR,
  insight_category VARCHAR,
  insight_title VARCHAR,
  insight_description TEXT,
  confidence_score DECIMAL,
  impact_score DECIMAL,
  recommendation TEXT,
  supporting_data JSONB
) AS $$
DECLARE
  v_booking_trend DECIMAL(8,2);
  v_customer_churn DECIMAL(5,2);
  v_equipment_utilization DECIMAL(5,2);
  v_peak_hours INTEGER[];
  v_top_equipment UUID[];
BEGIN
  -- Calculate booking trend
  SELECT
    CASE
      WHEN COUNT(*) > 0 THEN
        ((COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE - INTERVAL '7 days')::DECIMAL /
          COUNT(*) FILTER (WHERE created_at BETWEEN CURRENT_DATE - INTERVAL '14 days' AND CURRENT_DATE - INTERVAL '7 days')) - 1) * 100
      ELSE 0
    END INTO v_booking_trend
  FROM bookings
  WHERE status IN ('confirmed', 'active', 'completed')
    AND created_at >= CURRENT_DATE - INTERVAL '14 days';

  -- Calculate customer retention
  SELECT
    (COUNT(DISTINCT customer_id) FILTER (WHERE last_booking >= CURRENT_DATE - INTERVAL '30 days')::DECIMAL /
     COUNT(DISTINCT customer_id) * 100) INTO v_customer_churn
  FROM customer_booking_analytics;

  -- Get equipment utilization
  SELECT AVG(utilization_rate) INTO v_equipment_utilization
  FROM equipment_lifecycle
  WHERE stage = 'active';

  RETURN QUERY
  -- Booking trend insight
  SELECT
    'Trend Analysis'::VARCHAR,
    'Bookings'::VARCHAR,
    'Booking Volume Trend'::VARCHAR,
    CASE
      WHEN v_booking_trend > 20 THEN 'Booking volume is increasing significantly'
      WHEN v_booking_trend > 0 THEN 'Booking volume is growing steadily'
      WHEN v_booking_trend > -10 THEN 'Booking volume is stable'
      ELSE 'Booking volume is declining'
    END,
    0.8::DECIMAL,
    ABS(v_booking_trend)::DECIMAL,
    CASE
      WHEN v_booking_trend > 20 THEN 'Consider expanding equipment inventory'
      WHEN v_booking_trend < -15 THEN 'Investigate reasons for decline and implement retention strategies'
      ELSE 'Continue current operations'
    END,
    jsonb_build_object('trend_percentage', v_booking_trend, 'period_days', 7)
  WHERE ABS(v_booking_trend) > 5

  UNION ALL

  -- Customer retention insight
  SELECT
    'Customer Analysis'::VARCHAR,
    'Retention'::VARCHAR,
    'Customer Retention Rate'::VARCHAR,
    'Customer retention rate is ' || ROUND(v_customer_churn, 1) || '%',
    CASE
      WHEN v_customer_churn >= 80 THEN 0.9
      WHEN v_customer_churn >= 60 THEN 0.7
      WHEN v_customer_churn >= 40 THEN 0.5
      ELSE 0.3
    END,
    (100 - v_customer_churn)::DECIMAL,
    CASE
      WHEN v_customer_churn < 60 THEN 'Implement customer retention programs'
      WHEN v_customer_churn < 80 THEN 'Monitor customer satisfaction and improve service'
      ELSE 'Excellent customer retention - maintain current standards'
    END,
    jsonb_build_object('retention_rate', v_customer_churn, 'period_days', 30)
  WHERE v_customer_churn < 90

  UNION ALL

  -- Equipment utilization insight
  SELECT
    'Asset Management'::VARCHAR,
    'Utilization'::VARCHAR,
    'Equipment Utilization Rate'::VARCHAR,
    'Average equipment utilization is ' || ROUND(v_equipment_utilization, 1) || '%',
    CASE
      WHEN v_equipment_utilization >= 80 THEN 0.9
      WHEN v_equipment_utilization >= 60 THEN 0.7
      WHEN v_equipment_utilization >= 40 THEN 0.5
      ELSE 0.3
    END,
    v_equipment_utilization::DECIMAL,
    CASE
      WHEN v_equipment_utilization > 85 THEN 'High utilization - consider expanding fleet'
      WHEN v_equipment_utilization < 50 THEN 'Low utilization - optimize pricing or marketing'
      ELSE 'Optimal utilization - maintain current strategy'
    END,
    jsonb_build_object('utilization_rate', v_equipment_utilization, 'benchmark', 70)
  WHERE v_equipment_utilization IS NOT NULL;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to create A/B test
CREATE OR REPLACE FUNCTION create_ab_test(
  p_test_name VARCHAR,
  p_feature_name VARCHAR,
  p_hypothesis TEXT,
  p_variants JSONB,
  p_traffic_split JSONB,
  p_primary_metric VARCHAR,
  p_secondary_metrics TEXT[] DEFAULT NULL,
  p_minimum_sample_size INTEGER DEFAULT 1000,
  p_confidence_level DECIMAL DEFAULT 0.95
)
RETURNS UUID AS $$
DECLARE
  v_test_id UUID;
  v_start_date TIMESTAMP;
BEGIN
  v_start_date := CURRENT_TIMESTAMP;

  INSERT INTO ab_tests (
    test_name,
    description,
    feature_name,
    hypothesis,
    status,
    start_date,
    variants,
    traffic_split,
    primary_metric,
    secondary_metrics,
    minimum_sample_size,
    confidence_level
  ) VALUES (
    p_test_name,
    'A/B test for ' || p_feature_name,
    p_feature_name,
    p_hypothesis,
    'running',
    v_start_date,
    p_variants,
    p_traffic_split,
    p_primary_metric,
    p_secondary_metrics,
    p_minimum_sample_size,
    p_confidence_level
  ) RETURNING id INTO v_test_id;

  -- Log the test creation
  INSERT INTO audit_logs (table_name, record_id, action, metadata)
  VALUES (
    'ab_tests',
    v_test_id,
    'create',
    jsonb_build_object(
      'test_name', p_test_name,
      'feature', p_feature_name,
      'variants', p_variants,
      'sample_size', p_minimum_sample_size,
      'confidence', p_confidence_level
    )
  );

  RETURN v_test_id;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to track A/B test participation
CREATE OR REPLACE FUNCTION track_ab_test_participation(
  p_test_id UUID,
  p_user_id UUID,
  p_variant VARCHAR,
  p_event_type VARCHAR,
  p_event_data JSONB DEFAULT NULL
)
RETURNS void AS $$
DECLARE
  v_test RECORD;
  v_participation_rate DECIMAL(5,4);
BEGIN
  -- Get test details
  SELECT * INTO v_test FROM ab_tests WHERE id = p_test_id AND status = 'running';

  IF NOT FOUND THEN
    RETURN; -- Test not found or not running
  END IF;

  -- Assign user to variant based on traffic split (simplified)
  -- In production, this would use proper randomization
  INSERT INTO conversion_funnels (
    funnel_name,
    user_id,
    current_stage,
    stage_progress,
    metadata
  ) VALUES (
    'ab_test_' || v_test.test_name,
    p_user_id,
    p_variant,
    jsonb_build_object('event_type', p_event_type, 'event_data', p_event_data),
    jsonb_build_object('test_id', p_test_id, 'variant', p_variant)
  );

  -- Update test participation metrics
  UPDATE ab_tests
  SET
    results = COALESCE(results, '{}'::jsonb) || jsonb_build_object(
      p_variant,
      COALESCE(results->p_variant, '{"participants": 0, "conversions": 0}'::jsonb) || jsonb_build_object(
        'participants',
        (COALESCE(results->p_variant->>'participants', '0')::INTEGER + 1)
      )
    ),
    updated_at = NOW()
  WHERE id = p_test_id;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function to analyze A/B test results
CREATE OR REPLACE FUNCTION analyze_ab_test_results(p_test_id UUID)
RETURNS TABLE (
  test_id UUID,
  test_name VARCHAR,
  variant VARCHAR,
  participants INTEGER,
  conversions INTEGER,
  conversion_rate DECIMAL,
  confidence_interval VARCHAR,
  statistical_significance DECIMAL,
  recommendation VARCHAR
) AS $$
DECLARE
  v_test RECORD;
  v_variant_data JSONB;
  v_participants INTEGER;
  v_conversions INTEGER;
  v_rate DECIMAL(5,4);
BEGIN
  SELECT * INTO v_test FROM ab_tests WHERE id = p_test_id;

  IF NOT FOUND THEN
    RETURN;
  END IF;

  -- Analyze each variant
  FOR v_variant_data IN SELECT * FROM jsonb_each(v_test.results)
  LOOP
    v_participants := (v_variant_data.value->>'participants')::INTEGER;
    v_conversions := COALESCE((v_variant_data.value->>'conversions')::INTEGER, 0);
    v_rate := CASE
      WHEN v_participants > 0 THEN (v_conversions::DECIMAL / v_participants) * 100
      ELSE 0
    END;

    RETURN QUERY SELECT
      p_test_id,
      v_test.test_name,
      v_variant_data.key,
      v_participants,
      v_conversions,
      v_rate,
      '¬±' || ROUND(1.96 * SQRT(v_rate * (100 - v_rate) / v_participants), 2) || '%' as confidence_interval,
      CASE
        WHEN v_participants >= v_test.minimum_sample_size THEN 0.95
        WHEN v_participants >= v_test.minimum_sample_size * 0.5 THEN 0.8
        ELSE 0.5
      END,
      CASE
        WHEN v_participants >= v_test.minimum_sample_size AND v_rate > (
          SELECT AVG((value->>'conversions')::DECIMAL / (value->>'participants')::DECIMAL) * 100
          FROM jsonb_each(v_test.results)
          WHERE key != v_variant_data.key
        ) * 1.05 THEN 'Consider implementing this variant'
        ELSE 'Continue testing'
      END;
  END LOOP;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create triggers for automatic data collection
CREATE TRIGGER api_analytics_trigger
  AFTER INSERT ON api_usage
  FOR EACH ROW
  EXECUTE FUNCTION track_api_analytics(
    NEW.endpoint,
    NEW.method,
    NEW.user_id,
    NEW.response_time_ms,
    NEW.status_code,
    NULL,
    NULL,
    NULL
  );

-- Create comprehensive monitoring dashboard
CREATE OR REPLACE VIEW monitoring_dashboard AS
SELECT
  -- System Health
  'Database Health' as category,
  'Connection Count' as metric,
  (SELECT COUNT(*)::TEXT FROM pg_stat_activity) as current_value,
  (SELECT COUNT(*)::TEXT FROM database_performance_metrics WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour' ORDER BY timestamp DESC LIMIT 1) as previous_value,
  'Active connections to database' as description,

  'Query Performance' as category,
  'Avg Query Time (ms)' as metric,
  (SELECT ROUND(AVG(execution_time_ms), 2)::TEXT FROM query_performance_log WHERE execution_start >= CURRENT_TIMESTAMP - INTERVAL '1 hour') as current_value,
  (SELECT ROUND(AVG(execution_time_ms), 2)::TEXT FROM query_performance_log WHERE execution_start BETWEEN CURRENT_TIMESTAMP - INTERVAL '25 hours' AND CURRENT_TIMESTAMP - INTERVAL '1 hour') as previous_value,
  'Average query execution time' as description,

  'API Performance' as category,
  'Error Rate (%)' as metric,
  (SELECT ROUND(
    (COUNT(*) FILTER (WHERE status_code >= 400)::DECIMAL / COUNT(*)::DECIMAL) * 100, 2
  )::TEXT FROM api_analytics WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour') as current_value,
  (SELECT ROUND(
    (COUNT(*) FILTER (WHERE status_code >= 400)::DECIMAL / COUNT(*)::DECIMAL) * 100, 2
  )::TEXT FROM api_analytics WHERE timestamp BETWEEN CURRENT_TIMESTAMP - INTERVAL '25 hours' AND CURRENT_TIMESTAMP - INTERVAL '1 hour') as previous_value,
  'Percentage of API requests with errors' as description,

  'Business Metrics' as category,
  'Active Bookings' as metric,
  (SELECT COUNT(*)::TEXT FROM bookings WHERE status IN ('confirmed', 'active')) as current_value,
  (SELECT COUNT(*)::TEXT FROM bookings WHERE status IN ('confirmed', 'active') AND start_date >= CURRENT_DATE - INTERVAL '1 day') as previous_value,
  'Currently active equipment rentals' as description,

  'Customer Engagement' as category,
  'Session Duration (min)' as metric,
  (SELECT ROUND(AVG(time_on_page_seconds) / 60.0, 2)::TEXT FROM customer_behavior_analytics WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour') as current_value,
  (SELECT ROUND(AVG(time_on_page_seconds) / 60.0, 2)::TEXT FROM customer_behavior_analytics WHERE timestamp BETWEEN CURRENT_TIMESTAMP - INTERVAL '25 hours' AND CURRENT_TIMESTAMP - INTERVAL '1 hour') as previous_value,
  'Average time users spend on the platform' as description,

  'Cost Management' as category,
  'Daily Costs ($)' as metric,
  (SELECT ROUND(SUM(total_cost), 2)::TEXT FROM usage_costs WHERE date = CURRENT_DATE - 1) as current_value,
  (SELECT ROUND(SUM(total_cost), 2)::TEXT FROM usage_costs WHERE date = CURRENT_DATE - 2) as previous_value,
  'Daily operational costs' as description;

-- Grant permissions for monitoring dashboard
GRANT SELECT ON monitoring_dashboard TO authenticated;

-- Create RLS policy for monitoring dashboard
CREATE POLICY "Admins can view monitoring dashboard" ON api_analytics
  FOR SELECT USING (
    EXISTS (
      SELECT 1 FROM users
      WHERE users.id = auth.uid()
      AND users.role IN ('admin', 'super_admin')
    )
  );

CREATE POLICY "Admins can view system monitoring" ON database_performance_metrics
  FOR SELECT USING (
    EXISTS (
      SELECT 1 FROM users
      WHERE users.id = auth.uid()
      AND users.role IN ('admin', 'super_admin')
    )
  );

ALTER VIEW monitoring_dashboard SET (security_barrier = true);

-- Create automated alert triggers
CREATE TRIGGER system_health_monitoring
  AFTER INSERT ON database_performance_metrics
  FOR EACH ROW
  EXECUTE FUNCTION evaluate_alert_rules();

CREATE TRIGGER api_monitoring_trigger
  AFTER INSERT ON api_analytics
  FOR EACH ROW
  EXECUTE FUNCTION evaluate_alert_rules();

-- Create comprehensive cost tracking
CREATE TRIGGER daily_cost_calculation
  AFTER INSERT ON query_performance_log
  FOR EACH STATEMENT
  EXECUTE FUNCTION calculate_usage_costs();

-- Create automated maintenance scheduling
CREATE OR REPLACE FUNCTION schedule_monitoring_maintenance()
RETURNS void AS $$
DECLARE
  v_next_check TIMESTAMP;
BEGIN
  v_next_check := CURRENT_TIMESTAMP + INTERVAL '1 hour';

  -- Schedule next monitoring check
  INSERT INTO notifications (
    user_id,
    type,
    status,
    priority,
    title,
    message,
    scheduled_for,
    metadata
  )
  SELECT
    u.id,
    'system',
    'pending',
    'low',
    'Automated Monitoring Check',
    'System performance monitoring and optimization check scheduled.',
    v_next_check,
    jsonb_build_object('automated', true, 'check_type', 'monitoring_maintenance')
  FROM users u
  WHERE u.role IN ('admin', 'super_admin')
  AND NOT EXISTS (
    SELECT 1 FROM notifications
    WHERE metadata->>'check_type' = 'monitoring_maintenance'
      AND scheduled_for > CURRENT_TIMESTAMP
      AND status = 'pending'
  );
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create final comprehensive validation
DO $$
DECLARE
  v_monitoring_table_count INTEGER;
  v_alert_rule_count INTEGER;
  v_cost_tracking_count INTEGER;
  v_analytics_count INTEGER;
BEGIN
  SELECT COUNT(*) INTO v_monitoring_table_count
  FROM information_schema.tables
  WHERE table_schema = 'public'
    AND table_name IN (
      'usage_costs', 'alert_rules', 'alert_incidents', 'system_metrics',
      'database_performance_metrics', 'api_analytics', 'customer_behavior_analytics',
      'error_logs', 'feature_usage_analytics', 'ab_tests', 'conversion_funnels'
    );

  SELECT COUNT(*) INTO v_alert_rule_count FROM alert_rules WHERE is_active = true;
  SELECT COUNT(*) INTO v_cost_tracking_count FROM usage_costs WHERE date >= CURRENT_DATE - INTERVAL '7 days';
  SELECT COUNT(*) INTO v_analytics_count FROM system_metrics WHERE created_at >= CURRENT_DATE - INTERVAL '1 hour';

  RAISE NOTICE 'üìä Monitoring and Analytics System Complete!';
  RAISE NOTICE '';
  RAISE NOTICE 'üîç Monitoring Capabilities:';
  RAISE NOTICE '- System Health: Real-time database performance tracking';
  RAISE NOTICE '- API Analytics: Request/response time and error monitoring';
  RAISE NOTICE '- Customer Behavior: User journey and conversion tracking';
  RAISE NOTICE '- Error Tracking: Comprehensive error logging and analysis';
  RAISE NOTICE '- Cost Monitoring: Usage-based cost calculation and optimization';
  RAISE NOTICE '- Alert System: Automated alerting for performance issues';
  RAISE NOTICE '';
  RAISE NOTICE 'üìà Analytics Features:';
  RAISE NOTICE '- A/B Testing: Framework for feature optimization';
  RAISE NOTICE '- Conversion Funnels: Customer journey analysis';
  RAISE NOTICE '- Business Intelligence: Automated insight generation';
  RAISE NOTICE '- Performance Optimization: Automated recommendations';
  RAISE NOTICE '- Cost Analysis: Usage pattern and cost optimization';
  RAISE NOTICE '';
  RAISE NOTICE 'üìä Implementation Stats:';
  RAISE NOTICE '- Monitoring Tables: %', v_monitoring_table_count;
  RAISE NOTICE '- Active Alert Rules: %', v_alert_rule_count;
  RAISE NOTICE '- Cost Data Points: %', v_cost_tracking_count;
  RAISE NOTICE '- Analytics Metrics: %', v_analytics_count;
  RAISE NOTICE '';
  RAISE NOTICE '‚úÖ Your platform now has enterprise-grade monitoring!';

  -- Log monitoring setup completion
  INSERT INTO audit_logs (table_name, record_id, action, metadata)
  VALUES (
    'system',
    '00000000-0000-0000-0000-000000000000',
    'create',
    jsonb_build_object(
      'monitoring_complete', true,
      'timestamp', NOW(),
      'monitoring_tables', v_monitoring_table_count,
      'alert_rules', v_alert_rule_count,
      'cost_tracking', v_cost_tracking_count,
      'analytics_metrics', v_analytics_count,
      'system_monitored', true
    )
  );
END $$;

